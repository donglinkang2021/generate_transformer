{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from data import StoryDataset\n",
    "train_dataset = StoryDataset('data\\story_genaration_dataset\\ROCStories_train.csv')\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "vocab = build_vocab_from_iterator(map(tokenizer, train_dataset), specials=['<unk>'])\n",
    "vocab.set_default_index(vocab['<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'a', 'student']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('I am a student')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1644, 51, 44, 9570]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab将文本转换为数字\n",
    "vocab(['here', 'is', 'an', 'example'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'is', 'an', 'example']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# vocab将数字转换为文本\n",
    "vocab.lookup_tokens([1644, 51, 44, 9570])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.utils.data import dataset\n",
    "def data_process(raw_text_iter: dataset.IterableDataset) -> Tensor:\n",
    "    \"\"\"Converts raw text into a flat Tensor.\"\"\"\n",
    "    data = [torch.tensor(vocab(tokenizer(item)), dtype=torch.long) for item in raw_text_iter]\n",
    "    return torch.cat(tuple(filter(lambda t: t.numel() > 0, data)))\n",
    "test_iter = StoryDataset('data\\story_genaration_dataset\\ROCStories_test.csv')\n",
    "test_data = data_process(test_iter)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "def batchify(data: Tensor, bsz: int) -> Tensor:\n",
    "    \"\"\"Divides the data into ``bsz`` separate sequences, removing extra elements\n",
    "    that wouldn't cleanly fit.\n",
    "\n",
    "    Arguments:\n",
    "        data: Tensor, shape ``[N]``\n",
    "        bsz: int, batch size\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape ``[N // bsz, bsz]``\n",
    "    \"\"\"\n",
    "    seq_len = data.size(0) // bsz\n",
    "    data = data[:seq_len * bsz]\n",
    "    data = data.view(bsz, seq_len).t().contiguous()\n",
    "    return data.to(device)\n",
    "eval_batch_size = 10\n",
    "test_data = batchify(test_data, eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "bptt = 35\n",
    "def get_batch(source: Tensor, i: int) -> Tuple[Tensor, Tensor]:\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: Tensor, shape ``[full_seq_len, batch_size]``\n",
    "        i: int\n",
    "\n",
    "    Returns:\n",
    "        tuple (data, target), where data has shape ``[seq_len, batch_size]`` and\n",
    "        target has shape ``[seq_len * batch_size]``\n",
    "    \"\"\"\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].reshape(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dongl\\.conda\\envs\\d2l\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "from model import TransformerModel\n",
    "ntokens = len(vocab)  # size of vocabulary\n",
    "emsize = 200  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in ``nn.TransformerEncoder``\n",
    "nlayers = 2  # number of ``nn.TransformerEncoderLayer`` in ``nn.TransformerEncoder``\n",
    "nhead = 2  # number of heads in ``nn.MultiheadAttention``\n",
    "dropout = 0.2  # dropout probability\n",
    "model = TransformerModel(ntokens, emsize, nhead, d_hid, nlayers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_5_epoch.pth\", map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35, 10])\n",
      "torch.Size([350])\n",
      "35\n",
      "torch.Size([35, 10, 22513])\n",
      "torch.Size([350, 22513])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(0, test_data.size(0) - 1, bptt):\n",
    "        data, targets = get_batch(test_data, i)\n",
    "        print(data.shape)    # seq_len, batch_size \n",
    "        print(targets.shape) # batch_size*seq_len, 1\n",
    "        seq_len = data.size(0)\n",
    "        print(seq_len)\n",
    "        output = model(data)\n",
    "        print(output.shape) # batch_size, seq_len, vocab_size\n",
    "        output_flat = output.view(-1, ntokens)\n",
    "        print(output_flat.shape) # batch_size*seq_len, vocab_size\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = data.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"at her grandmother ' s house . a light shone in the window where her grandmother sat like a <unk> . her grandmother hugged beth tightly , relieved she had arrived safely . one day\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab.lookup_tokens(tmp_data[1].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "greedy search\n",
      "11\n",
      "prompt sentence: \n",
      "tommy was very close to his dad and loved him greatly\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output sentence: \n",
      "tommy was very close to his dad and loved him greatly . he was in the water . he was in the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water . he got out of the water\n",
      "beam search\n",
      "input sentence: \n",
      "a little fish bubble\n",
      "output sentence: \n",
      "a little fish bubble bottle of sugar . he was in the middle of her . she was in her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her house . she got out of her . she was\n"
     ]
    }
   ],
   "source": [
    "from decode import greedy_search, beam_search\n",
    "print(\"greedy search\")\n",
    "prompt = \"tommy was very close to his dad and loved him greatly\"\n",
    "# Example usage:\n",
    "greedy_search(\n",
    "    model, \n",
    "    prompt, \n",
    "    tokenizer, \n",
    "    vocab,\n",
    "    max_len=100\n",
    ")\n",
    "print(\"beam search\")\n",
    "prompt = \"a little fish bubble\"\n",
    "# Example usage:\n",
    "beam_search(\n",
    "    model, \n",
    "    prompt, \n",
    "    tokenizer, \n",
    "    vocab,\n",
    "    max_len=100,\n",
    "    beam_width=3\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
